{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Four: Build a Regressor\n",
    "\n",
    "Expected time to complete: XX minutes\n",
    "\n",
    "## Goal of this Course\n",
    "During this course we will build a regressor to forecast the minutes watched within the last two-week group.\n",
    "\n",
    "This course is split into the following parts:\n",
    "- <a href='#context'>Context</a> \n",
    "- <a href='#model_evaluation'>Model Evaluation</a> \n",
    "- <a href='#baseline_forecast'>Baseline Forecast</a>\n",
    "    - <a href='#load_data'>Load the Data</a>\n",
    "    - <a href='#define_baseline'>Define a Baseline</a>\n",
    "\n",
    "\n",
    "- <a href='#linear_reg'>Linear Regression</a>\n",
    "    - <a href='#model_training'>Model Training</a>\n",
    "    - <a href='#out_sample_error'>Out-of-Sample Error</a>\n",
    "    - <a href='#in_sample_error'>In-Sample Error and Estimated Coefficients</a>\n",
    "\n",
    "\n",
    "- <a href='#ridge_reg'>Ridge Regression</a>\n",
    "    - <a href='#model_training2'>Model Training and Hyperparameters Tuning</a>\n",
    "    - <a href='#best_ridge_reg'>Best Ridge Regression</a>\n",
    "\n",
    "\n",
    "- <a href='#lasso_reg'>Lasso Regression</a>\n",
    "    - <a href='#model_training3'>Model Training and Hyperparameters Tuning</a>\n",
    "    - <a href='#best_lasso_reg'>Best Lasso Regression</a>\n",
    "\n",
    "<a id='context'></a>\n",
    "# Context\n",
    "In the previous tutorial we explained how the training process works and how to evaluate our models in the classification framework. We built few models to predict whether a user will consume content on the last two-week group based on his past behaviour on the 7 two-week groups before. \n",
    "\n",
    "Forecasting if a user will consume content is very usefull. It allows for example to focus our efforts on the ones who are not likely to come back.\n",
    "\n",
    "The users who are likely to come back within the last two-week period might also have different levels of engagement. Understanding to what extent they will consume content is key for the BBC to personnalise their experience with the product.   \n",
    "\n",
    "In order to forecast the minutes watched within the next two-week group we will first define our evaluation metric. As for the classifier, we will then try different models and compare their performances to a baseline. \n",
    "\n",
    "\n",
    "<a id='model_evaluation'></a>\n",
    "# Model Evaluation\n",
    "\n",
    "For regressors, the evaluation of the model is less straightforward and is usually based upon the average residual between the actual observations and the model predictions. \n",
    "\n",
    "__Root mean squared error__ (RMSE) and __mean absolute error__ (MAE) are two commonly used examples of these. Once we have a representation of the error produced by the model we can compare that error with the error of the most simple model of the data (usually the mean). This statistic is known as the coefficient of determination or __R-squared__ and represents the part of variability the model managed to get.\n",
    "\n",
    "<a id='baseline_forecast'></a>\n",
    "# Baseline Forecast\n",
    "\n",
    "<a id='load_data'></a>\n",
    "## Load the Data\n",
    "The first thing to do is to get our data back and make sure we have the correct one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We put both target arrays (regression and classification) in the same txt file\n",
    "# As both target arrays have the same size we just need to split it it two\n",
    "# and get the correct part for the prediction task\n",
    "target = np.split(np.loadtxt('target.txt'), 2)[0].flatten()\n",
    "features = pd.read_csv('features.csv')\n",
    "\n",
    "# User id as index\n",
    "features = features.set_index('user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tw_lag7_watched</th>\n",
       "      <th>tw_lag6_watched</th>\n",
       "      <th>tw_lag5_watched</th>\n",
       "      <th>tw_lag4_watched</th>\n",
       "      <th>tw_lag3_watched</th>\n",
       "      <th>tw_lag2_watched</th>\n",
       "      <th>tw_lag1_watched</th>\n",
       "      <th>average_completion</th>\n",
       "      <th>total_sessions</th>\n",
       "      <th>num_genre</th>\n",
       "      <th>...</th>\n",
       "      <th>most_weekday_weekday_1</th>\n",
       "      <th>most_weekday_weekday_2</th>\n",
       "      <th>most_weekday_weekday_3</th>\n",
       "      <th>most_weekday_weekday_4</th>\n",
       "      <th>most_weekday_weekday_5</th>\n",
       "      <th>most_weekday_weekday_6</th>\n",
       "      <th>most_timeday_Afternoon</th>\n",
       "      <th>most_timeday_Evening</th>\n",
       "      <th>most_timeday_Morning</th>\n",
       "      <th>most_timeday_Night</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0001c6</th>\n",
       "      <td>16.679200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.371496</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000c1a</th>\n",
       "      <td>0.162867</td>\n",
       "      <td>0.147467</td>\n",
       "      <td>107.0984</td>\n",
       "      <td>145.686233</td>\n",
       "      <td>2.286283</td>\n",
       "      <td>100.487767</td>\n",
       "      <td>132.432083</td>\n",
       "      <td>0.233136</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>001c53</th>\n",
       "      <td>1.866300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.309867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.489419</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>001d44</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>14.547700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.248017</td>\n",
       "      <td>0.058203</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>002b2e</th>\n",
       "      <td>291.477033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.228233</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tw_lag7_watched  tw_lag6_watched  tw_lag5_watched  tw_lag4_watched  \\\n",
       "user_id                                                                       \n",
       "0001c6         16.679200         0.000000           0.0000         0.000000   \n",
       "000c1a          0.162867         0.147467         107.0984       145.686233   \n",
       "001c53          1.866300         0.000000           0.0000         0.000000   \n",
       "001d44          0.000000         0.000000           0.0000        14.547700   \n",
       "002b2e        291.477033         0.000000           0.0000         0.000000   \n",
       "\n",
       "         tw_lag3_watched  tw_lag2_watched  tw_lag1_watched  \\\n",
       "user_id                                                      \n",
       "0001c6          0.000000         0.152550         0.000000   \n",
       "000c1a          2.286283       100.487767       132.432083   \n",
       "001c53          1.309867         0.000000         0.000000   \n",
       "001d44          0.000000         0.000000         0.248017   \n",
       "002b2e          0.000000         0.000000         0.000000   \n",
       "\n",
       "         average_completion  total_sessions  num_genre         ...          \\\n",
       "user_id                                                        ...           \n",
       "0001c6             0.371496               2          1         ...           \n",
       "000c1a             0.233136              28          5         ...           \n",
       "001c53             0.489419               3          2         ...           \n",
       "001d44             0.058203               2          2         ...           \n",
       "002b2e             0.228233              17          5         ...           \n",
       "\n",
       "         most_weekday_weekday_1  most_weekday_weekday_2  \\\n",
       "user_id                                                   \n",
       "0001c6                        1                       0   \n",
       "000c1a                        0                       0   \n",
       "001c53                        0                       1   \n",
       "001d44                        0                       0   \n",
       "002b2e                        0                       1   \n",
       "\n",
       "         most_weekday_weekday_3  most_weekday_weekday_4  \\\n",
       "user_id                                                   \n",
       "0001c6                        0                       0   \n",
       "000c1a                        1                       0   \n",
       "001c53                        0                       0   \n",
       "001d44                        0                       0   \n",
       "002b2e                        0                       0   \n",
       "\n",
       "         most_weekday_weekday_5  most_weekday_weekday_6  \\\n",
       "user_id                                                   \n",
       "0001c6                        0                       0   \n",
       "000c1a                        0                       0   \n",
       "001c53                        0                       0   \n",
       "001d44                        0                       1   \n",
       "002b2e                        0                       0   \n",
       "\n",
       "         most_timeday_Afternoon  most_timeday_Evening  most_timeday_Morning  \\\n",
       "user_id                                                                       \n",
       "0001c6                        0                     1                     0   \n",
       "000c1a                        0                     0                     1   \n",
       "001c53                        0                     0                     1   \n",
       "001d44                        0                     0                     1   \n",
       "002b2e                        0                     1                     0   \n",
       "\n",
       "         most_timeday_Night  \n",
       "user_id                      \n",
       "0001c6                    0  \n",
       "000c1a                    0  \n",
       "001c53                    0  \n",
       "001d44                    0  \n",
       "002b2e                    0  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure we have the right input database\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.44833333e-01,   3.18047633e+02,   1.98035000e+00,\n",
       "         1.00590667e+01,   0.00000000e+00,   4.79261667e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure we have the right output\n",
    "target[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='define_baseline'></a>\n",
    "## Define a Baseline\n",
    "\n",
    "As mentionned in the last course we should have a baseline to compare the performance of our models with. Here as we are forecasting a quantitative variable, a simple model is for example the one that predicts a constant value like 0, or the mean or the median we do observe in our data. \n",
    "\n",
    "Note as well that by default `scikit` _maximises_ metrics and error metrics have been implemented as a negative number within the scoring function. We therefore negate the errors we get to make them comparable and are looking for those models that maximise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a baseline to compare our results to (mean and median minutes watched and 0)\n",
    "mean=np.mean(target)\n",
    "median=np.median(target)\n",
    "\n",
    "mean_forecast=[mean]*len(target)\n",
    "median_forecast=[median]*len(target)\n",
    "zero_forecast=[0]*len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 75.9168097123\n",
      "Median: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean: \"+str(mean))\n",
    "print(\"Median: \"+str(median))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median is null, meaning than more than half of our users don't consume content on iPlayer within the last two weeks.\n",
    "\n",
    "There is no need to consider the constant model equal to 0 as a baseline here (same as the median one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score if we forecast the mean: -115.319756974\n",
      "Score if we forecast the median: -75.9168097123\n"
     ]
    }
   ],
   "source": [
    "# Compute the errors for these different baseline\n",
    "from sklearn import metrics\n",
    "print(\"Score if we forecast the mean:\",\n",
    "      -metrics.mean_absolute_error(target,mean_forecast))\n",
    "print(\"Score if we forecast the median:\",\n",
    "      -metrics.mean_absolute_error(target,median_forecast))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try different kind of regressions. These models are great for business insights as we can __quantify the impact__ of our input variables on the target one.\n",
    "\n",
    "<a id='linear_reg'></a>\n",
    "# Linear Regression\n",
    "\n",
    "The easiest way to relate our output variable to our input ones is to use a __linear combination__.\n",
    "\n",
    "A linear regression decomposes the observed output in an additive way such that the coefficients we estimate in the training process can be interpreted as the impact of a given input all other things being equal (AOTBE). For more details see: https://en.wikipedia.org/wiki/Linear_regression\n",
    "\n",
    "And for the `scikit learn` documentation: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's build a simple regression\n",
    "from sklearn import linear_model\n",
    "\n",
    "# We will use cross validation, so import helper functions for this\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "# Plots\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model_training'></a>\n",
    "## Model Training\n",
    "\n",
    "With this kind of modelling there is no hyperparameter to tune. Indeed, the model only has to evaluate the _contribution_ of each input variable to the linear combination.  \n",
    "\n",
    "The `cross_val_predicts` functions here returns, for each element in the input, the prediction that was obtained for that element when it was in the test set during the cross-validation process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We setup a simple linear regression, again using cross validation\n",
    "reg=linear_model.LinearRegression()\n",
    "\n",
    "predicted=cross_val_predict(reg, features,target)\n",
    "scores=cross_val_score(reg, features, target, scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='out_sample_error'></a>\n",
    "## Out-of-Sample Error\n",
    "As for the classification part, we need to take the average of the different errors obtained during the cross-validation process to compute an out-of-sample error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: -58.0733721136\n"
     ]
    }
   ],
   "source": [
    "# Compute the mean error obtained in the CV\n",
    "print(\"Mean score:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, there is an improvement compared to the constant model equal to 0 (median baseline).\n",
    "\n",
    "We can also plot our predicted values against the output we actually observe. The more the dots are close to the first bisector, the better are our forecasts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEKCAYAAADaa8itAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110a3b940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's compare graphically the predicted and actual values\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(target, predicted)\n",
    "ax.plot([target.min(), target.max()], [target.min(), target.max()], 'k--', lw=1)\n",
    "ax.set_xlabel('Observed')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite difficult to visually assess the performance of our model, but the main trend is toward the first bisector which is a good sign.\n",
    "\n",
    "It also looks like there is an outlier we forgot to remove here (dot on the right up side of the graph). \n",
    "\n",
    "<a id='in_sample_error'></a>\n",
    "## In-Sample Error and Estimated Coefficients\n",
    "Let's now train on our entire dataset and have a look at the estimated coefficients to get the patterns the model detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's train on all data \n",
    "lin_reg=linear_model.LinearRegression()\n",
    "mod1=lin_reg.fit(features[subset_features],target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-56.600083327149065"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In-sample error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "-metrics.mean_absolute_error(target,mod1.predict(features[subset_features]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remind that it is usual to have a smaller in-sample error as the output data we are comparing our forecasts with has been used to train the model.\n",
    "\n",
    "For the estimated coefficients we should in theory have a look at the __p-values__ - statistic values that indicate whether our estimated coefficient is significantly different from zero. We won't go more into details here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.541515</td>\n",
       "      <td>most_weekday_weekday_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.831388</td>\n",
       "      <td>most_genre_Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.625429</td>\n",
       "      <td>most_genre_Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.676683</td>\n",
       "      <td>most_timeday_Afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.654163</td>\n",
       "      <td>most_genre_News</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   coefficient                 feature\n",
       "0     6.541515  most_weekday_weekday_2\n",
       "1     4.831388        most_genre_Sport\n",
       "2     3.625429        most_genre_Drama\n",
       "3     2.676683  most_timeday_Afternoon\n",
       "4     2.654163         most_genre_News"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a look at the estimated coefficients\n",
    "coef1=pd.DataFrame(\n",
    "    {'feature': list(features[subset_features].columns),\n",
    "     'coefficient': list(mod1.coef_)\n",
    "    })\n",
    "coef1.sort_values(by='coefficient', ascending=False).reset_index(drop=True).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-6.872885</td>\n",
       "      <td>most_genre_Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-7.023103</td>\n",
       "      <td>most_timeday_Evening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-8.022035</td>\n",
       "      <td>most_timeday_Night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-9.211985</td>\n",
       "      <td>most_weekday_weekday_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-16.821276</td>\n",
       "      <td>most_genre_Children's</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    coefficient                 feature\n",
       "29    -6.872885        most_genre_Music\n",
       "30    -7.023103    most_timeday_Evening\n",
       "31    -8.022035      most_timeday_Night\n",
       "32    -9.211985  most_weekday_weekday_0\n",
       "33   -16.821276   most_genre_Children's"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef1.sort_values(by='coefficient', ascending=False).reset_index(drop=True).tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are quite interesting here. It seems that some particular habits __drive__ the amount of content consumed. User that consume content on iPlayer most of the time on Wednesdays, or in the afternoon, or whose favourite genre is News are likely to consume more content (positive coefficients). On the other side, users whose favourite genre is Children's won't consume - in terms of total of minutes - much. It's difficult to speak of drivers for _engagement_ here because the minutes watched is highly correlated to the type of content. Children's pieces of content are on average shorter than Sport or Drama ones. It doesn't mean than the user's who consume only Children's content are less engaged. \n",
    "\n",
    "The estimated coefficients can be interpreted as follow: for the `num_genre` variable, the estimated contribution is 1,3. All other thing being equal (favourite genre \"Drama\", favourite time of the day \"Evening\" etc.), consuming one other genre in the past weeks will make the user consume 1,3mn more within the two following weeks. While this is certainly a bit far-fetched, looking at the sign of the coefficient is a good sanity check. For example we won't expect the number of different genre consumed to lower engagement.\n",
    "\n",
    "We also have to bare in mind that we didn't check the _p-values_ here and that the coefficients could not be significant.\n",
    "\n",
    "<a id='ridge_reg'></a>\n",
    "# Ridge Regression\n",
    "\n",
    "__DETAILS __\n",
    "\n",
    "`scikit documentation`: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n",
    "<a id='model_training2'></a>\n",
    "## Model Training and Hyperparameters Tuning\n",
    "\n",
    "We need to tune the penalty parameter _alpha_. Let's have the same approach than for the classification part and do a grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup the model and develop a simple grid search against some key parameters\n",
    "param_alpha=[0.001,0.01,0.1,1.0,10,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's keep track of our best parameters\n",
    "best_score=-200\n",
    "best_param=0\n",
    "\n",
    "# we will setup a manual grid search, but you can also use the gridsearchCV capability in sklearn\n",
    "for i in param_alpha:\n",
    "    reg_r = linear_model.Ridge(alpha = i)\n",
    "    scores=cross_val_score(reg_r,\n",
    "                           features,\n",
    "                           target,\n",
    "                           scoring='neg_mean_absolute_error')\n",
    "    if np.mean(scores)>best_score:\n",
    "        best_score=np.mean(scores)\n",
    "        best_param=i\n",
    "\n",
    "# print the overall best results\n",
    "print('Best Settings: alpha:',best_param)\n",
    "print('Score:', best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__COMMENTS__\n",
    "\n",
    "<a id='best_ridge_reg'></a>\n",
    "## Best Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's keep our best model (hyperparameters tuned)\n",
    "ridge=linear_model.Ridge(alpha = best_param)\n",
    "mod2=ridge.fit(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In-sample error\n",
    "-metrics.mean_absolute_error(target,mod2.predict(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's compare graphically the predicted and actual values\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(target, mod2.predict(features))\n",
    "ax.plot([target.min(), target.max()], [target.min(), target.max()], 'k--', lw=1)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__COMMENTS__\n",
    "\n",
    "<a id='lasso_reg'></a>\n",
    "# Lasso Regression\n",
    "\n",
    "__DETAILS __\n",
    "\n",
    "`scikit documentation`: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "\n",
    "<a id='model_training3'></a>\n",
    "## Model Training and Hyperparameters Tuning\n",
    "\n",
    "Again we need to tune the penalty parameter _alpha_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup the model and develop a simple grid search against some key parameters\n",
    "param_alpha=[0.001,0.01,0.1,1.0,10,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's keep track of our best parameters\n",
    "best_score=-200\n",
    "best_param=0\n",
    "\n",
    "# we will setup a manual grid search, but you can also use the gridsearchCV capability in sklearn\n",
    "for i in param_alpha:\n",
    "    reg_r = linear_model.LassoLars(alpha = i)\n",
    "    scores=cross_val_score(reg_r,\n",
    "                           features,\n",
    "                           target,\n",
    "                           scoring='neg_mean_absolute_error')\n",
    "    if np.mean(scores)>best_score:\n",
    "        best_score=np.mean(scores)\n",
    "        best_param=i\n",
    "\n",
    "# print the overall best results\n",
    "print('Best Settings: alpha:',best_param)\n",
    "print('Score:', best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__COMMENTS__\n",
    "\n",
    "<a id='best_lasso_reg'></a>\n",
    "## Best Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's keep our best model (hyperparameters tuned)\n",
    "lasso=linear_model.LassoLars(alpha = best_param)\n",
    "mod3=lasso.fit(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In-sample error\n",
    "-metrics.mean_absolute_error(target,mod3.predict(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's compare graphically the predicted and actual values\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(target, mod3.predict(features))\n",
    "ax.plot([target.min(), target.max()], [target.min(), target.max()], 'k--', lw=1)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__COMMENTS__\n",
    "\n",
    "<a id='combine'></a>\n",
    "# Combine the Regression and Classification Models\n",
    "and generate an overall score \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
