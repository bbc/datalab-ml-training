{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context\n",
    "\n",
    "## Quick reminder on last course\n",
    "\n",
    "XX\n",
    "\n",
    "## Goal of this course\n",
    "\n",
    "XX\n",
    "\n",
    "\n",
    "# Concepts\n",
    "\n",
    "## Estimate the parameters\n",
    "\n",
    "As we are in the supervised framework, each model will try here to relate the target output y - what we want to forecast, to the features X - what we can observe, using more or less complex formulas. These formulas will contains parameters that we need to estimate.\n",
    "\n",
    "For all algorithms we will actually minimize __the loss function__ - which quantifies how far our predictions on X will be with our model and its set of parameters to the correct output y. The estimation process can be tricky as we are considering complex relations between our features and output. Fortunately, Python is doing all the maths for us! If you want to learn more on this: __XX (SGD etc.)__\n",
    "\n",
    "\n",
    "## Training and test sets\n",
    " \n",
    "When training machine learning models - i.e. estimating the parameters, we want to avoid training the model on all of the possible data that we have available. This is to avoid creating a model that is to specifically atuned to our training data and will later not generalise - this is often called __overfitting__. \n",
    "\n",
    "So instead we will spilt our data into __training and test sets__. The state of the art is to do __cross-validation__. Basically we will split the dataset in _n_ (usually 5) chunks and:\n",
    "- train our model on the features dataset consolidated with _(n-1)_ chunks (the training set);\n",
    "- compute the error, i.e. evaluate its predictions against the target y - that we _do_ observe, on the remaining chunk (the test set);\n",
    "- and reiterate this process for all the possible combinations of _(n-1)_ vs 1 chunks of data (_n_ combinations).\n",
    "\n",
    "At the end we then have computed _n_ errors (see the model evaluation part for more details on the metrics used). We will take the average of the errors for model selection purposes.\n",
    "\n",
    "For many complex problems and datasets the 'bleeding' of knowledge from the evaluation set into the training set can be a real problem. In that case our model will perform much worse in production than what we would have assumed. And so it is really important to make sure that we don't have information in the training set that we would not have been able to have at that time.\n",
    "\n",
    "What kind of modelling?\n",
    "\n",
    "## Model Selection\n",
    " \n",
    "The first decision you need to make in the model selection process in the supervised framework is whether you plan to use a __classifier__ or a __regressor model__. Classifiers make discrete predictions about a datapoint into a finite number of classes while regressors make linear predictions.  \n",
    " \n",
    "Different models work in different ways and are more or less suitable for different problems. Fortunately, however,  understanding these specific differences is not essential to solve your data problems. The python module `scikit learn` contains all of the models that you are likely to need and the format of the data it requires is standardised across models. This makes it very easy to try your data using a myriad of different models and choose the one that performs best on your data.\n",
    " \n",
    "In our current project we use both classifiers and regressors to predict engagement. In the classifier we simply try to predict whether someone has watched any content in the two-week period while in the regressor we attempt to predict the number of minutes watched by the viewer within the two-week period.\n",
    "\n",
    "We won't tacke both prediction tasks here. This course focused on classification and the next one will focus on regression.\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "To evaluate our models there are different metrics we can use. For classifiers we can use __the accuracy__ (percentage correct) or __the ROC curve__. The accuracy is the simplest metric but it gives us little insight into the behaviour of the model. ROC curves (and __the area under the curve__ statistic) give us a greater understanding of the separability of the data. For more details see https://en.wikipedia.org/wiki/Receiver_operating_characteristic.\n",
    "\n",
    "We will evaluate our model in different stages on the model selection process. First, when training our model we will compute a test or __out-of-sample error__ (actually _n_ ones using cross-validation). It's called out-of-sample here because the predictions are made on data the model has never seen before. \n",
    "\n",
    "Usually models have also __hyperparameters__ - parameters that we don't need to estimate but to tune during the training process. A good way of doing that is to do a grid search and to evaluate the model with the different combinations of hyperparameters. We will then retain the one with the lowest error. \n",
    "\n",
    "Once we have selected the best set of hyperparameters for a given model, or the best kind of model for the data we have gor, we can retrain it on the entire dataset and compute its __in-of-sample__ error, i.e. evaluate its predictions with the observed targets, keeping in mind that we have used this data to build the model this time.   \n",
    "\n",
    "Finally, when computing the performance of a model based on a given metric it's important to bare in mind what the performance of a simple model would be. Usually we consider the completely random one as a baseline. Any model you build must be evaluated in terms of improvement over this performance. \n",
    "\n",
    "# Classification\n",
    "\n",
    "The first thing to do is to get our data back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We put both target arrays (regression and classification) in the same txt file\n",
    "# As both target arrays have the same size we just need to split it it two\n",
    "# and get the correct part for the prediction task\n",
    "target = np.split(np.loadtxt('target.txt'), 2)[1].flatten()\n",
    "features = pd.read_csv('features.csv')\n",
    "\n",
    "# User id as index\n",
    "features = features.set_index('user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tw_lag7_watched</th>\n",
       "      <th>tw_lag6_watched</th>\n",
       "      <th>tw_lag5_watched</th>\n",
       "      <th>tw_lag4_watched</th>\n",
       "      <th>tw_lag3_watched</th>\n",
       "      <th>tw_lag2_watched</th>\n",
       "      <th>tw_lag1_watched</th>\n",
       "      <th>average_completion</th>\n",
       "      <th>total_sessions</th>\n",
       "      <th>total_watched</th>\n",
       "      <th>...</th>\n",
       "      <th>most_weekday_weekday_1</th>\n",
       "      <th>most_weekday_weekday_2</th>\n",
       "      <th>most_weekday_weekday_3</th>\n",
       "      <th>most_weekday_weekday_4</th>\n",
       "      <th>most_weekday_weekday_5</th>\n",
       "      <th>most_weekday_weekday_6</th>\n",
       "      <th>most_timeday_Afternoon</th>\n",
       "      <th>most_timeday_Evening</th>\n",
       "      <th>most_timeday_Morning</th>\n",
       "      <th>most_timeday_Night</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0001c6</th>\n",
       "      <td>16.679200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.371496</td>\n",
       "      <td>2</td>\n",
       "      <td>16.831750</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000c1a</th>\n",
       "      <td>0.162867</td>\n",
       "      <td>0.147467</td>\n",
       "      <td>107.0984</td>\n",
       "      <td>145.686233</td>\n",
       "      <td>2.286283</td>\n",
       "      <td>100.487767</td>\n",
       "      <td>132.432083</td>\n",
       "      <td>0.233136</td>\n",
       "      <td>28</td>\n",
       "      <td>488.301100</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>001c53</th>\n",
       "      <td>1.866300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.309867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.489419</td>\n",
       "      <td>3</td>\n",
       "      <td>3.176167</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>001d44</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>14.547700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.248017</td>\n",
       "      <td>0.058203</td>\n",
       "      <td>2</td>\n",
       "      <td>14.795717</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>002b2e</th>\n",
       "      <td>291.477033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.228233</td>\n",
       "      <td>17</td>\n",
       "      <td>291.477033</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tw_lag7_watched  tw_lag6_watched  tw_lag5_watched  tw_lag4_watched  \\\n",
       "user_id                                                                       \n",
       "0001c6         16.679200         0.000000           0.0000         0.000000   \n",
       "000c1a          0.162867         0.147467         107.0984       145.686233   \n",
       "001c53          1.866300         0.000000           0.0000         0.000000   \n",
       "001d44          0.000000         0.000000           0.0000        14.547700   \n",
       "002b2e        291.477033         0.000000           0.0000         0.000000   \n",
       "\n",
       "         tw_lag3_watched  tw_lag2_watched  tw_lag1_watched  \\\n",
       "user_id                                                      \n",
       "0001c6          0.000000         0.152550         0.000000   \n",
       "000c1a          2.286283       100.487767       132.432083   \n",
       "001c53          1.309867         0.000000         0.000000   \n",
       "001d44          0.000000         0.000000         0.248017   \n",
       "002b2e          0.000000         0.000000         0.000000   \n",
       "\n",
       "         average_completion  total_sessions  total_watched  \\\n",
       "user_id                                                      \n",
       "0001c6             0.371496               2      16.831750   \n",
       "000c1a             0.233136              28     488.301100   \n",
       "001c53             0.489419               3       3.176167   \n",
       "001d44             0.058203               2      14.795717   \n",
       "002b2e             0.228233              17     291.477033   \n",
       "\n",
       "                ...          most_weekday_weekday_1  most_weekday_weekday_2  \\\n",
       "user_id         ...                                                           \n",
       "0001c6          ...                               1                       0   \n",
       "000c1a          ...                               0                       0   \n",
       "001c53          ...                               0                       1   \n",
       "001d44          ...                               0                       0   \n",
       "002b2e          ...                               0                       1   \n",
       "\n",
       "         most_weekday_weekday_3  most_weekday_weekday_4  \\\n",
       "user_id                                                   \n",
       "0001c6                        0                       0   \n",
       "000c1a                        1                       0   \n",
       "001c53                        0                       0   \n",
       "001d44                        0                       0   \n",
       "002b2e                        0                       0   \n",
       "\n",
       "         most_weekday_weekday_5  most_weekday_weekday_6  \\\n",
       "user_id                                                   \n",
       "0001c6                        0                       0   \n",
       "000c1a                        0                       0   \n",
       "001c53                        0                       0   \n",
       "001d44                        0                       1   \n",
       "002b2e                        0                       0   \n",
       "\n",
       "         most_timeday_Afternoon  most_timeday_Evening  most_timeday_Morning  \\\n",
       "user_id                                                                       \n",
       "0001c6                        0                     1                     0   \n",
       "000c1a                        0                     0                     1   \n",
       "001c53                        0                     0                     1   \n",
       "001d44                        0                     0                     1   \n",
       "002b2e                        0                     1                     0   \n",
       "\n",
       "         most_timeday_Night  \n",
       "user_id                      \n",
       "0001c6                    0  \n",
       "000c1a                    0  \n",
       "001c53                    0  \n",
       "001d44                    0  \n",
       "002b2e                    0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline score\n",
    "\n",
    "Usually dealing with 0/1 classification problem we talk about __scoring__. And the probability to belong to the class 1 (usually our class of interest) is the score.\n",
    "\n",
    "As mentionned before we should have a baseline to compare the performance of our models with. We usually choose as a baseline score the one obtained by a random allocation of our users. Allocating randomly 100 users to the class 1, the accuracy will depend on the effective concentration of class 1 in the entire population. If we observe 30% of class 1 in the population, then we should have 30 correct predictions out of our 100 users labeled class 1 with this random allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40551224332930713"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check our baseline score\n",
    "sum(target)/len(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for any classification model to add value we would like it to perform with an accuracy of more than 40% (otherwise guessing based on the proportions would be a better model).\n",
    "\n",
    "## Tree based classification model\n",
    "\n",
    "Scikit documentation: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's build a simple tree based classification model\n",
    "from sklearn import tree\n",
    "\n",
    "# Accuracy as our error evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# We will use cross validation, so import helper functions for this\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training + HYPERPAM TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup the model and develop a simple grid search against some key parameters\n",
    "param_max_depth=[2,3,4,6,8,10]\n",
    "param_min_leaf=[75,90,100,110,125,150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Settings: Max Depth: 4 - Min Sample Leaf: 100\n",
      "Score: 0.794992688868\n"
     ]
    }
   ],
   "source": [
    "# Let's keep track of our best parameters\n",
    "best_score=0\n",
    "best_param=[0,0]\n",
    "\n",
    "# We will use the itertools library to try all the possible combinations of paramaters\n",
    "# We could also have used the gridsearchCV capability in scikit learn\n",
    "for c in itertools.product(param_max_depth,param_min_leaf):\n",
    "    treeclass=tree.DecisionTreeClassifier(max_depth=c[0],min_samples_leaf=c[1])\n",
    "    scores=cross_val_score(treeclass,\n",
    "                           features,\n",
    "                           target,\n",
    "                           scoring='accuracy')\n",
    "    if np.mean(scores)>best_score:\n",
    "        best_score=np.mean(scores)\n",
    "        best_param=c\n",
    "\n",
    "# print the overall best results\n",
    "print('Best Settings: Max Depth:',best_param[0], '- Min Sample Leaf:',best_param[1])\n",
    "print('Score:', best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best tree based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's keep our best model (hyperparameters tuned)\n",
    "treeclass=tree.DecisionTreeClassifier(max_depth=best_param[0],\n",
    "                                      min_samples_leaf=best_param[1])\n",
    "mod1=treeclass.fit(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79795761502141205"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod1.score(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tw_lag1_watched</td>\n",
       "      <td>0.720014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>number_watched</td>\n",
       "      <td>0.115590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tw_lag3_watched</td>\n",
       "      <td>0.090706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tw_lag2_watched</td>\n",
       "      <td>0.054184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tw_lag5_watched</td>\n",
       "      <td>0.012255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>num_genre</td>\n",
       "      <td>0.006670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>average_completion</td>\n",
       "      <td>0.000581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>most_weekday_weekday_4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>most_genre_Weather</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>most_weekday_weekday_0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  feature  importance\n",
       "0         tw_lag1_watched    0.720014\n",
       "1          number_watched    0.115590\n",
       "2         tw_lag3_watched    0.090706\n",
       "3         tw_lag2_watched    0.054184\n",
       "4         tw_lag5_watched    0.012255\n",
       "5               num_genre    0.006670\n",
       "6      average_completion    0.000581\n",
       "7  most_weekday_weekday_4    0.000000\n",
       "8      most_genre_Weather    0.000000\n",
       "9  most_weekday_weekday_0    0.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_imp1=pd.DataFrame(\n",
    "    {'feature': list(features.columns),\n",
    "     'importance': list(mod1.feature_importances_)\n",
    "    })\n",
    "feature_imp1.sort_values(by='importance', ascending=False).reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forrest\n",
    "\n",
    "Scikit documentation: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's build a random forrest\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training + HYPERPAM TUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup the model and develop a simple grid search against some key parameters\n",
    "param_max_depth=[2,3,4,6,8,10]\n",
    "param_min_leaf=[75,90,100,110,125,150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Settings: Max Depth: 10 - Min Sample Leaf: 75\n",
      "Score: 0.795102410105\n"
     ]
    }
   ],
   "source": [
    "# Let's keep track of our best parameters\n",
    "best_score=0\n",
    "best_param=[0,0]\n",
    "\n",
    "# We will use the itertools library to try all the possible combinations of paramaters\n",
    "# We could also have used the gridsearchCV capability in scikit learn\n",
    "for c in itertools.product(param_max_depth,param_min_leaf):\n",
    "    forrestclass=RandomForestClassifier(n_estimators=200,\n",
    "                                        max_depth=c[0],min_samples_leaf=c[1])\n",
    "    scores=cross_val_score(forrestclass,\n",
    "                           features,\n",
    "                           target,\n",
    "                           scoring='accuracy')\n",
    "    if np.mean(scores)>best_score:\n",
    "        best_score=np.mean(scores)\n",
    "        best_param=c\n",
    "\n",
    "# print the overall best results\n",
    "print('Best Settings: Max Depth:',best_param[0], '- Min Sample Leaf:',best_param[1])\n",
    "print('Score:', best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best RF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's keep our best model (hyperparameters tuned)\n",
    "forrestclass=RandomForestClassifier(n_estimators=200,\n",
    "                                    max_depth=best_param[0],\n",
    "                                    min_samples_leaf=best_param[1])\n",
    "mod2=forrestclass.fit(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80213022949379598"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod2.score(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tw_lag1_watched</td>\n",
       "      <td>0.233214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tw_lag2_watched</td>\n",
       "      <td>0.183893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>total_sessions</td>\n",
       "      <td>0.102758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>number_watched</td>\n",
       "      <td>0.090360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tw_lag3_watched</td>\n",
       "      <td>0.084441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>num_weekday</td>\n",
       "      <td>0.073745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>num_genre</td>\n",
       "      <td>0.057644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>total_watched</td>\n",
       "      <td>0.049236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tw_lag5_watched</td>\n",
       "      <td>0.030680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tw_lag4_watched</td>\n",
       "      <td>0.029978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           feature  importance\n",
       "0  tw_lag1_watched    0.233214\n",
       "1  tw_lag2_watched    0.183893\n",
       "2   total_sessions    0.102758\n",
       "3   number_watched    0.090360\n",
       "4  tw_lag3_watched    0.084441\n",
       "5      num_weekday    0.073745\n",
       "6        num_genre    0.057644\n",
       "7    total_watched    0.049236\n",
       "8  tw_lag5_watched    0.030680\n",
       "9  tw_lag4_watched    0.029978"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_imp2=pd.DataFrame(\n",
    "    {'feature': list(features.columns),\n",
    "     'importance': list(mod2.feature_importances_)\n",
    "    })\n",
    "feature_imp2.sort_values(by='importance', ascending=False).reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Scikit documentation: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's try something that is not based on decision tree\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training + HYPERPAM tun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup the model and develop a simple grid search against some key parameters\n",
    "param_C=[0.001,0.01,0.1,1.0,10,100,1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Settings: C: 0.01\n",
      "Score: 0.773690489886\n"
     ]
    }
   ],
   "source": [
    "# Let's keep track of our best parameters\n",
    "best_score=0\n",
    "best_param=0\n",
    "\n",
    "# we will setup a manual grid search, but you can also use the gridsearchCV capability in sklearn\n",
    "for i in param_C:\n",
    "    logclass=linear_model.LogisticRegression(C=i)\n",
    "    scores=cross_val_score(logclass,\n",
    "                           features,\n",
    "                           target,\n",
    "                           scoring='accuracy')\n",
    "    if np.mean(scores)>best_score:\n",
    "        best_score=np.mean(scores)\n",
    "        best_param=i\n",
    "\n",
    "# print the overall best results\n",
    "print('Best Settings: C:',best_param)\n",
    "print('Score:', best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's keep our best model (hyperparameters tuned)\n",
    "logclass=linear_model.LogisticRegression(C=best_param)\n",
    "mod3=logclass.fit(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77709454265949274"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod3.score(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.254606</td>\n",
       "      <td>num_weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.113189</td>\n",
       "      <td>num_genre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.007645</td>\n",
       "      <td>number_watched</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005754</td>\n",
       "      <td>tw_lag1_watched</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001347</td>\n",
       "      <td>tw_lag2_watched</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       coef          feature\n",
       "0  0.254606      num_weekday\n",
       "1  0.113189        num_genre\n",
       "2  0.007645   number_watched\n",
       "3  0.005754  tw_lag1_watched\n",
       "4  0.001347  tw_lag2_watched"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_mod3=pd.DataFrame(\n",
    "    {'feature': list(features.columns),\n",
    "     'coef': list(mod3.coef_.flatten())\n",
    "    })\n",
    "coef_mod3.sort_values(by='coef',ascending=False).reset_index(drop=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.282503</td>\n",
       "      <td>most_genre_Factual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.283514</td>\n",
       "      <td>most_genre_Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-0.289396</td>\n",
       "      <td>most_timeday_Afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.305260</td>\n",
       "      <td>most_timeday_Night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.394988</td>\n",
       "      <td>most_timeday_Evening</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        coef                 feature\n",
       "31 -0.282503      most_genre_Factual\n",
       "32 -0.283514        most_genre_Drama\n",
       "33 -0.289396  most_timeday_Afternoon\n",
       "34 -0.305260      most_timeday_Night\n",
       "35 -0.394988    most_timeday_Evening"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_mod3.sort_values(by='coef',ascending=False).reset_index(drop=True).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P VALUES ??\n",
    "AND which threshold ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)\n",
    "\n",
    "Scikit documentation: http://scikit-learn.org/stable/modules/svm.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's try to get some non linear patterns\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training + HYPERPAM TUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup the model and develop a simple grid search against some key parameters\n",
    "param_C=[0.001,0.01,0.1,1.0,10,100,1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Settings: C: 1.0\n",
      "Score: 0.692433069322\n"
     ]
    }
   ],
   "source": [
    "# Let's keep track of our best parameters\n",
    "best_score=0\n",
    "best_param=0\n",
    "\n",
    "# we will setup a manual grid search, but you can also use the gridsearchCV capability in sklearn\n",
    "for i in param_C:\n",
    "    svcclass=svm.SVC(C=i)\n",
    "    scores=cross_val_score(svcclass,\n",
    "                           features,\n",
    "                           target,\n",
    "                           scoring='accuracy')\n",
    "    if np.mean(scores)>best_score:\n",
    "        best_score=np.mean(scores)\n",
    "        best_param=i\n",
    "\n",
    "# print the overall best results\n",
    "print('Best Settings: C:',best_param)\n",
    "print('Score:', best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's keep our best model (hyperparameters tuned)\n",
    "svcclass=svm.SVC(C=best_param)\n",
    "mod4=svcclass.fit(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94729329087515102"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod4.score(features,target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
